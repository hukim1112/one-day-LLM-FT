{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/one-day-LLM-FT/blob/main/Alpaca_LLaMa_instruction_fintuning_torch_style.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
      "metadata": {
        "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3"
      },
      "source": [
        "# From Llama to Alpaca: Finetunning and LLM with Weights & Biases\n",
        "\n",
        "이 Notebook에서는 사전 훈련된 LLama 모델을 인스트럭션 데이터셋에 대해 미세 조정(fine-tuning)하는 방법을 배울 것입니다. davinci-003 (GPT-3)으로 생성된 데이터 대신 GPT-4를 사용하여 더욱 향상된 인스트럭션 데이터셋을 활용하는 업데이트된 버전의 Alpaca 데이터셋을 사용합니다. 자세한 내용은 [공식 저장소 페이지](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data)를 참조하세요.\n",
        "\n",
        "이 Notebook은 최소 24GB 메모리를 갖춘 A100/A10 GPU가 필요합니다. 매개변수를 조정하여 T4에서 실행할 수도 있지만 실행 시간이 매우 길어집니다.\n",
        "\n",
        "이 Notebook에는 연관 프로젝트 및 보고서: [wandb](wandb.me/alpaca)가 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ef319f-bf26-4192-8951-8d536181ab67",
      "metadata": {
        "id": "03ef319f-bf26-4192-8951-8d536181ab67"
      },
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!pip install wandb\n",
        "!pip install transformers\n",
        "!pip install accelerate -U\n",
        "!pip install trl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7804f904-5746-4530-867d-c766f4501dea",
      "metadata": {
        "id": "7804f904-5746-4530-867d-c766f4501dea"
      },
      "source": [
        "## Prepare your Instruction Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da04c0a5-f481-4364-880d-10c254388987",
      "metadata": {
        "id": "da04c0a5-f481-4364-880d-10c254388987"
      },
      "source": [
        "알파카 (GPT-4 curated instructions and outputs) 데이터셋을 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
      "metadata": {
        "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0665a80-6137-4a61-a3da-93bde606df04",
      "metadata": {
        "id": "b0665a80-6137-4a61-a3da-93bde606df04"
      },
      "source": [
        "데이터셋을 로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zKGbqe450SI4",
      "metadata": {
        "id": "zKGbqe450SI4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"alpaca_gpt4_data.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6XvtiMRbF097",
      "metadata": {
        "id": "6XvtiMRbF097"
      },
      "source": [
        "데이터셋의 구조"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vVWCGCLZFxsV",
      "metadata": {
        "id": "vVWCGCLZFxsV"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tkgsRp7TF5OG",
      "metadata": {
        "id": "tkgsRp7TF5OG"
      },
      "source": [
        "하나의 샘플을 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gMZA6TyhF5Wz",
      "metadata": {
        "id": "gMZA6TyhF5Wz"
      },
      "outputs": [],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zMXIcvb8HUGg",
      "metadata": {
        "id": "zMXIcvb8HUGg"
      },
      "outputs": [],
      "source": [
        "dataset['train'][9]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e596e369-56aa-4721-9271-6686eed8fb35",
      "metadata": {
        "id": "e596e369-56aa-4721-9271-6686eed8fb35"
      },
      "source": [
        "데이터셋에는 명령(instruction)과 결과(output)가 포함되어 있습니다. 모델은 다음 토큰을 예측하도록 훈련되므로, 한 가지 방법은 단순히 둘을 연결(concatenate)하고 그 결과를 토대로 모델을 훈련하는 것입니다. 이상적으로 프롬프트는 입력과 출력 위치를 명확하게 표시하는 방식으로 구성되어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d",
      "metadata": {
        "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d"
      },
      "outputs": [],
      "source": [
        "def prompt_no_input(example):\n",
        "    return (\"Below is an instruction that describes a task. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024aec96-40fb-4417-8e64-060a301b0f0b",
      "metadata": {
        "id": "024aec96-40fb-4417-8e64-060a301b0f0b"
      },
      "outputs": [],
      "source": [
        "row = dataset['train'][0]\n",
        "print(prompt_no_input(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037",
      "metadata": {
        "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037"
      },
      "source": [
        "어떤 instruction은 input 변수 안에 context가 들어있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b795343f-0356-4689-8bc6-9ac650716c8b",
      "metadata": {
        "id": "b795343f-0356-4689-8bc6-9ac650716c8b"
      },
      "outputs": [],
      "source": [
        "def prompt_input(example):\n",
        "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6",
      "metadata": {
        "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6"
      },
      "outputs": [],
      "source": [
        "row = dataset['train'][9]\n",
        "print(prompt_input(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a",
      "metadata": {
        "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a"
      },
      "source": [
        "일단은 프롬프트를 처리합니다. 나중에 적절한 양의 패딩(padding)과 함께 결과를 추가할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f",
      "metadata": {
        "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f"
      },
      "source": [
        "input이 있는 케이스와 없는 케이스를 통합하는 함수를 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cq3jyI8swNSO",
      "metadata": {
        "id": "Cq3jyI8swNSO"
      },
      "outputs": [],
      "source": [
        "def create_alpaca_prompt(example):\n",
        "    example['prompt'] = prompt_no_input(example) if example[\"input\"] == \"\" else prompt_input(example)\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f-H6Cd1s7E",
      "metadata": {
        "id": "70f-H6Cd1s7E"
      },
      "outputs": [],
      "source": [
        "prompt_dataset = dataset.map(create_alpaca_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DAh3RSBuIzEO",
      "metadata": {
        "id": "DAh3RSBuIzEO"
      },
      "outputs": [],
      "source": [
        "prompt_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813",
      "metadata": {
        "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813"
      },
      "outputs": [],
      "source": [
        "print(prompt_dataset['train']['prompt'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc69153b-eb20-4f6d-afba-5b737d36320b",
      "metadata": {
        "id": "cc69153b-eb20-4f6d-afba-5b737d36320b"
      },
      "source": [
        "가끔씩 아래처럼 종료 토큰(EOS)을 수동으로 주는 예제들이 있습니다. 이는 Fine-tuning 중에 문제를 발생시킬 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "351f519a-7234-4817-a6ee-4d7418f234fe",
      "metadata": {
        "id": "351f519a-7234-4817-a6ee-4d7418f234fe"
      },
      "outputs": [],
      "source": [
        "def pad_eos(example):\n",
        "    EOS_TOKEN = \"</s>\"\n",
        "    example['answer'] = f\"{example['output']}{EOS_TOKEN}\"\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aac1c00-6cc7-4b65-a972-e0ba5a2d6504",
      "metadata": {
        "id": "3aac1c00-6cc7-4b65-a972-e0ba5a2d6504"
      },
      "source": [
        "- 예제 텍스트\n",
        "\n",
        "  \n",
        "text = \"\\<s>This is a sample sentence.\\</s>\"\n",
        "\n",
        "- 토큰화\n",
        "tokenized_output = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "print(tokenized_output)\n",
        "\n",
        "=> {'input_ids': tensor([[ 1, 1, 4013, 338, 263, 4559, 10541, 21106, 29879, 29958]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
        "\n",
        "- decoding을 해보면,\n",
        "tokenizer.decode([ 1, 1, 4013, 338, 263, 4559, 10541, 21106, 29879, 29958])\n",
        "\n",
        "\"\\<s>This is a sample sentence.\\</s>\"\n",
        "\n",
        "예제를 보면 알 수 있듯이\n",
        "하지만 \\</s>이 29879, 29958로 인코딩 되고 있습니다. 즉 예상치 못한 id 값으로 변경되고 있습니다.\n",
        "우리가 기대한 EOS token id는 2번입니다. 실제로 token id 2의 decoding 결과는 \\</s>가 맞습니다.\n",
        "\n",
        "정리하자면 문자열은 여러 방식으로 tokenization 될 수 있기 때문에 special token의 경우 정해진 token id로 변경되는 게 중요하므로 유의해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4",
      "metadata": {
        "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qul4fye5wNT-",
      "metadata": {
        "id": "Qul4fye5wNT-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "063Ax0luKJgy",
      "metadata": {
        "id": "063Ax0luKJgy"
      },
      "source": [
        "최종적으로 유저 prompt와 모델 answer를 합칩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802",
      "metadata": {
        "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802"
      },
      "outputs": [],
      "source": [
        "def get_example(example):\n",
        "    example['example'] = example['prompt'] + example['output']\n",
        "    return example\n",
        "\n",
        "\n",
        "final_dataset = prompt_dataset.map(get_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad92d95-23d5-474c-9271-59948d5dcbb0",
      "metadata": {
        "id": "bad92d95-23d5-474c-9271-59948d5dcbb0"
      },
      "source": [
        "이것이 모델이 보고 배울 필요가 있는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R16Ge7dqJ3oz",
      "metadata": {
        "id": "R16Ge7dqJ3oz"
      },
      "outputs": [],
      "source": [
        "print(final_dataset['train']['example'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd",
      "metadata": {
        "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd"
      },
      "source": [
        "## Converting text to numbers: Tokenizer\n",
        "\n",
        "우리는 데이터셋을 토큰들로 변환할 필요가 있습니다. 이것은 transformers의 tokenizer로 쉽게 달성할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
      "metadata": {
        "id": "720c707b-3bce-4164-b8c1-3c3122200c39"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gd05dHqPL2Ie",
      "metadata": {
        "id": "Gd05dHqPL2Ie"
      },
      "outputs": [],
      "source": [
        "model_id = 'NousResearch/Llama-2-7b-chat-hf'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ZKPEegTL0p1",
      "metadata": {
        "id": "5ZKPEegTL0p1"
      },
      "outputs": [],
      "source": [
        "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
        "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wInLJDPBL718",
      "metadata": {
        "id": "wInLJDPBL718"
      },
      "source": [
        "많은 튜토리얼이 아래와 같은 방법을 추천하지만,\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "이 경우 학습 시 pad token이 무시되면서 eos token 무시되면서 모델이 문장의 끝을 학습하기 어려워지는 문제가 있음."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DREOc9z0MKfw",
      "metadata": {
        "id": "DREOc9z0MKfw"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token_id = 0 #추론 시에는 tokenizer.eos_token_id로 지정해도 상관 없음.\n",
        "\n",
        "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
        "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "337fedfd-e238-4e86-a96b-24dfeed11f8a",
      "metadata": {
        "id": "337fedfd-e238-4e86-a96b-24dfeed11f8a"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a99cdc97-0cb6-4e5e-ac9e-27c7cf22d46d",
      "metadata": {
        "id": "a99cdc97-0cb6-4e5e-ac9e-27c7cf22d46d"
      },
      "outputs": [],
      "source": [
        "tokenizer.add_eos_token = True # 이제 tokenizer는 eos token을 자동으로 추가합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "461efa85-67f3-4c12-a7d4-d4e6deb020b2",
      "metadata": {
        "id": "461efa85-67f3-4c12-a7d4-d4e6deb020b2"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6dd79e4-b4da-4d5f-9c41-404bea2f9160",
      "metadata": {
        "id": "c6dd79e4-b4da-4d5f-9c41-404bea2f9160"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode([1, 1619, 15729, 526, 2675, 4549, 29991, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c69466-f7e6-45b8-a167-718c80cedc0f",
      "metadata": {
        "id": "20c69466-f7e6-45b8-a167-718c80cedc0f"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\", padding='max_length', max_length=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1e5e29-254a-4dbf-ac02-ea6bb2a16e77",
      "metadata": {
        "id": "cb1e5e29-254a-4dbf-ac02-ea6bb2a16e77"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\",\n",
        "                 padding='max_length',\n",
        "                 max_length=10,\n",
        "                 return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a89a61d-34b7-4a56-b1d8-98ebcf3384d7",
      "metadata": {
        "id": "0a89a61d-34b7-4a56-b1d8-98ebcf3384d7"
      },
      "outputs": [],
      "source": [
        "tokenizer([\"My experiments are going strong!\",\n",
        "           \"I love Llamas\"],\n",
        "          padding='max_length',\n",
        "          # padding='longest',\n",
        "          max_length=10,\n",
        "          return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qJ__Hh4Ofx4O",
      "metadata": {
        "id": "qJ__Hh4Ofx4O"
      },
      "outputs": [],
      "source": [
        "x = tokenizer([\"My experiments are going strong!\",\n",
        "           \"I love Llamas\"],\n",
        "          padding='max_length',\n",
        "          # padding='longest',\n",
        "          max_length=10,\n",
        "          return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lt4qnlmMf0KO",
      "metadata": {
        "id": "lt4qnlmMf0KO"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(x['input_ids'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BAjWPwxB4Csz",
      "metadata": {
        "id": "BAjWPwxB4Csz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IK8RnHwg4CvV",
      "metadata": {
        "id": "IK8RnHwg4CvV"
      },
      "outputs": [],
      "source": [
        "for i, example in enumerate(final_dataset['train']['example'][0:3]):\n",
        "    print(f\"---------{i+1}번째 데이터 샘플--------------\")\n",
        "    print(example)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4RT8bdJO3Nd2",
      "metadata": {
        "id": "4RT8bdJO3Nd2"
      },
      "source": [
        "## Data collator\n",
        "\n",
        "Causal language modeling을 위해 우리는 동적 마스킹 모드를 off한 DataCollatorForLanguageModeling(tokenizer, mlm=False)를 사용하여 GPT 계열 모델을 학습시킬 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZTODLkVQ4SIM",
      "metadata": {
        "id": "ZTODLkVQ4SIM"
      },
      "source": [
        "### DataCollatorForLanguageModeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Pt_dQmaIl5t",
      "metadata": {
        "id": "9Pt_dQmaIl5t"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "causal_model_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KIxpzTMoIl8S",
      "metadata": {
        "id": "KIxpzTMoIl8S"
      },
      "outputs": [],
      "source": [
        "out = causal_model_collator([tokenizer(example) for example in final_dataset['train']['example'][:1]])\n",
        "for key in out:\n",
        "    print(f\"{key} : {out[key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tQA-NjC6Il-d",
      "metadata": {
        "id": "tQA-NjC6Il-d"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(out['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QI5hBhiO4amK",
      "metadata": {
        "id": "QI5hBhiO4amK"
      },
      "source": [
        "### 미세조정을 위한 DataCollatorForCompletionOnlyLM 사용"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m_dp8U173ren",
      "metadata": {
        "id": "m_dp8U173ren"
      },
      "source": [
        "DataCollatorForLanguageModeling를 사용하면 모델은 사용자의 입력 첫 번째 토큰부터 다음 토큰을 예측하도록 학습하게 됩니다. 그러나 실제로 원하는 것은 모델이 명령어가 주어졌을 때 응답을 생성하는 법을 배우도록 하는 것입니다. 이를 위해서는 사용자의 입력을 마스킹하여, 모델 학습 중에 이 입력들이 손실(loss)에 기여하지 않도록 해야 합니다. 명령어 LLM 미세 조정에서도 마찬가지입니다. 모델이 명령어의 다음 토큰을 예측하도록 학습하려는 것이 아니라, 명령어를 마스킹하고 모델이 응답을 예측하도록 학습해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bKd3LT1mNX47",
      "metadata": {
        "id": "bKd3LT1mNX47"
      },
      "outputs": [],
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "\n",
        "response_template = \"Response:\"\n",
        "completion_only_collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hFM7gc_-NX7q",
      "metadata": {
        "id": "hFM7gc_-NX7q"
      },
      "outputs": [],
      "source": [
        "out = completion_only_collator([tokenizer(example) for example in final_dataset['train']['example'][:1]])\n",
        "for key in out:\n",
        "    print(f\"{key} : {out[key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RggmKG-kNX-Z",
      "metadata": {
        "id": "RggmKG-kNX-Z"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(out['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iSnO_gW8PL26",
      "metadata": {
        "id": "iSnO_gW8PL26"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d8c8e9-73b2-4c28-846d-3ae850134a18",
      "metadata": {
        "id": "c8d8c8e9-73b2-4c28-846d-3ae850134a18"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "CZm7t1RL4L80",
      "metadata": {
        "id": "CZm7t1RL4L80"
      },
      "source": [
        "## 데이터 스플릿"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3mAGmEVM0SLD",
      "metadata": {
        "id": "3mAGmEVM0SLD"
      },
      "outputs": [],
      "source": [
        "train_test_split = final_dataset['train'].train_test_split(test_size=1000)\n",
        "# x = final_dataset['train'].train_test_split(test_size=100)\n",
        "# train_test_split = x['test'].train_test_split(test_size=20)\n",
        "# train_test_split\n",
        "# batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MUz4MPz_0SNR",
      "metadata": {
        "id": "MUz4MPz_0SNR"
      },
      "outputs": [],
      "source": [
        "# 분할된 데이터셋을 확인합니다.\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "print(f\"훈련 데이터 개수: {len(train_dataset)}\")\n",
        "print(f\"평가 데이터 개수: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I3k3xqsZ4kna",
      "metadata": {
        "id": "I3k3xqsZ4kna"
      },
      "source": [
        "## Tokenization 전처리와 Dataloader 연결"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tSWw-mqDg47C",
      "metadata": {
        "id": "tSWw-mqDg47C"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = train_dataset.map(lambda x : tokenizer(x['example']))\n",
        "tokenized_eval_dataset = eval_dataset.map(lambda x : tokenizer(x['example']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jG4Zs-rEh--k",
      "metadata": {
        "id": "jG4Zs-rEh--k"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['instruction', 'input', 'output', 'prompt', 'example'])\n",
        "tokenized_eval_dataset = tokenized_eval_dataset.remove_columns(['instruction', 'input', 'output', 'prompt', 'example'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aqezXdMkgfsK",
      "metadata": {
        "id": "aqezXdMkgfsK"
      },
      "source": [
        "토큰화된 데이터셋을 dataloader와 연결해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vIxMYiynyqhE",
      "metadata": {
        "id": "vIxMYiynyqhE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "batch_size = 16  # I have an A100 GPU with 40GB of RAM 😎\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=causal_model_collator,\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_eval_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=causal_model_collator,\n",
        "    shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BG6XMsRcyqjT",
      "metadata": {
        "id": "BG6XMsRcyqjT"
      },
      "outputs": [],
      "source": [
        "b = next(iter(train_dataloader))\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eTep0z2lyqlh",
      "metadata": {
        "id": "eTep0z2lyqlh"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(b[\"input_ids\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XVlqlKTlyqoE",
      "metadata": {
        "id": "XVlqlKTlyqoE"
      },
      "outputs": [],
      "source": [
        "# -100을 제외한 토큰만 필터링\n",
        "valid_labels = [token_id for token_id in b[\"labels\"][0] if token_id != -100]\n",
        "\n",
        "# 유효한 토큰을 decode\n",
        "decoded_text = tokenizer.decode(valid_labels)\n",
        "\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sNqlCQ3UlP7V",
      "metadata": {
        "id": "sNqlCQ3UlP7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0zJKkvf7lP9w",
      "metadata": {
        "id": "0zJKkvf7lP9w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "RDCzzQ-29KNh",
      "metadata": {
        "id": "RDCzzQ-29KNh"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hxeYSaMC9KNh",
      "metadata": {
        "id": "hxeYSaMC9KNh"
      },
      "source": [
        "다음과 같이 모든 하이퍼파라미터들을 관리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V__oSZ799KNh",
      "metadata": {
        "id": "V__oSZ799KNh"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "config = SimpleNamespace(\n",
        "    project_name='meta-llama-ft-alpaca',\n",
        "    run_name='Causal-language-modeling',\n",
        "    model_id=model_id,\n",
        "    dataset_name=\"alpaca-gpt4\",\n",
        "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
        "    n_freeze=24,  # How many layers we don't train, LLama 7B has 32.\n",
        "    lr=2e-4,\n",
        "    n_eval_samples=10, # How many samples to generate on validation\n",
        "    epochs=3,  # we do 3 pasess over the dataset.\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
        "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training\n",
        "    log_model=False,  # upload the model to W&B?\n",
        "    gradient_checkpointing = True,  # saves even more memory\n",
        "    freeze_embed = True,  # why train this? let's keep them frozen ❄️\n",
        "    seed=seed,\n",
        ")\n",
        "\n",
        "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JOW-3sjl9KNh",
      "metadata": {
        "id": "JOW-3sjl9KNh"
      },
      "outputs": [],
      "source": [
        "print(f\"We will train for {config.total_train_steps} steps and evaluate every epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LKj0gF-t9KNh",
      "metadata": {
        "id": "LKj0gF-t9KNh"
      },
      "source": [
        "pretrained model을 가져옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KsKyp3Ao9KNh",
      "metadata": {
        "id": "KsKyp3Ao9KNh"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_id,\n",
        "    device_map=0,\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    use_cache=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IutRmWXX9KNh",
      "metadata": {
        "id": "IutRmWXX9KNh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def param_count(m):\n",
        "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
        "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
        "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
        "    return params, trainable_params\n",
        "\n",
        "params, trainable_params = param_count(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C1DNSEko9KNh",
      "metadata": {
        "id": "C1DNSEko9KNh"
      },
      "source": [
        "전체 모델을 학습하는 것은 강력한 연산력과 메모리를 필요로하기 때문에 우리는 8개의 layer를 튜닝할 것 입니다. LLama는 총 32개를 가지고 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vr39Gdt19KNi",
      "metadata": {
        "id": "Vr39Gdt19KNi"
      },
      "outputs": [],
      "source": [
        "# freeze layers (disable gradients)\n",
        "for param in model.parameters(): param.requires_grad = False\n",
        "for param in model.lm_head.parameters(): param.requires_grad = True\n",
        "for param in model.model.layers[config.n_freeze:].parameters(): param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JmHxbYJ29KNi",
      "metadata": {
        "id": "JmHxbYJ29KNi"
      },
      "outputs": [],
      "source": [
        "# Just freeze embeddings for small memory decrease\n",
        "if config.freeze_embed:\n",
        "    model.model.embed_tokens.weight.requires_grad_(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ReVioN69KNi",
      "metadata": {
        "id": "6ReVioN69KNi"
      },
      "source": [
        "또한 그래디언트 체크포인팅을 사용하여 더 많이 저장할 수도 있습니다(이것은 훈련을 느리게 만들지만, 얼마나 느려질지는 여러분의 특정 설정에 따라 달라집니다). 대용량 모델을 메모리에 맞추는 방법에 대해 허깅페이스 웹사이트에 [좋은 아티클](https://huggingface.co/docs/transformers/v4.18.0/en/performance)이 있으니 확인해 보시길 권장합니다!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vpmm8SI_9KNi",
      "metadata": {
        "id": "vpmm8SI_9KNi"
      },
      "outputs": [],
      "source": [
        "# save more memory\n",
        "if config.gradient_checkpointing:\n",
        "    model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25QIm0yQ9KNi",
      "metadata": {
        "id": "25QIm0yQ9KNi"
      },
      "outputs": [],
      "source": [
        "params, trainable_params = param_count(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ol4k95ZN9KNi",
      "metadata": {
        "id": "Ol4k95ZN9KNi"
      },
      "source": [
        "### Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YRcKwYyI9KNi",
      "metadata": {
        "id": "YRcKwYyI9KNi"
      },
      "outputs": [],
      "source": [
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9,0.99), eps=1e-5)\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optim,\n",
        "    num_training_steps=config.total_train_steps,\n",
        "    num_warmup_steps=config.total_train_steps // 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xAiICm3J9KNi",
      "metadata": {
        "id": "xAiICm3J9KNi"
      },
      "source": [
        "## Testing during training\n",
        "\n",
        "거의 다 왔습니다, 이제 모델에서 샘플링하는 간단한 함수를 만들어 가끔 모델이 출력하는 것을 시각적으로 확인해 봅시다! 간단하게 모델.generate 메소드를 감싸 보겠습니다. GenerationConfig에서 기본 샘플링 매개변수를 가져와 해당 모델 ID를 전달하면 됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qMQZ9Gg_9KNi",
      "metadata": {
        "id": "qMQZ9Gg_9KNi"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
        "test_config = SimpleNamespace(\n",
        "    max_new_tokens=256,\n",
        "    gen_config=gen_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hse5ft8Y9KNi",
      "metadata": {
        "id": "Hse5ft8Y9KNi"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
        "    tokenizer.add_eos_token = False\n",
        "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(tokenized_prompt,\n",
        "                            max_new_tokens=max_new_tokens,\n",
        "                            generation_config=gen_config)\n",
        "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44aNuoyS9KNi",
      "metadata": {
        "id": "44aNuoyS9KNi"
      },
      "source": [
        "LoL 🤷"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6PyMJtNA9KNi",
      "metadata": {
        "id": "6PyMJtNA9KNi"
      },
      "outputs": [],
      "source": [
        "prompt = eval_dataset[14][\"prompt\"]\n",
        "print(prompt + generate(prompt, 128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5123cec1-63f1-4b47-b773-021842b2dd57",
      "metadata": {
        "id": "5123cec1-63f1-4b47-b773-021842b2dd57"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "oiL1PIcT9KNi",
      "metadata": {
        "id": "oiL1PIcT9KNi"
      },
      "source": [
        "우리는 그 결과를 n 단계마다 프로젝트에 테이블로 기록할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SxZQ-jAh9KNi",
      "metadata": {
        "id": "SxZQ-jAh9KNi"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def prompt_table(examples, log=False, table_name=\"predictions\"):\n",
        "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"output\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
        "\n",
        "    for prompt, gpt4_output in tqdm(zip(examples['prompt'], examples['output']), leave=False):\n",
        "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
        "        table.add_data(prompt, out, prompt+out, gpt4_output, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
        "    if log:\n",
        "        wandb.log({table_name:table})\n",
        "    return table\n",
        "\n",
        "def to_gpu(tensor_dict):\n",
        "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
        "\n",
        "class Accuracy:\n",
        "    \"A simple Accuracy function compatible with HF models\"\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.tp = 0.\n",
        "    def update(self, logits, labels):\n",
        "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
        "        tp = (logits == labels).sum()\n",
        "        self.count += len(logits)\n",
        "        self.tp += tp\n",
        "        return tp / len(logits)\n",
        "    def compute(self):\n",
        "        return self.tp / self.count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vvUS5CJV9KNi",
      "metadata": {
        "id": "vvUS5CJV9KNi"
      },
      "source": [
        "원하신다면 검증을 빠르게 추가할 수도 있습니다. 이 단계에서 테이블을 생성할 수도 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tj57fWkm9KNi",
      "metadata": {
        "id": "Tj57fWkm9KNi"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate():\n",
        "    model.eval();\n",
        "    eval_acc = Accuracy()\n",
        "    loss, total_steps = 0., 0\n",
        "    for step, batch in enumerate(pbar:=tqdm(eval_dataloader, leave=False)):\n",
        "        if \"length\" in batch:\n",
        "            del batch[\"length\"]\n",
        "        pbar.set_description(f\"doing validation\")\n",
        "        batch = to_gpu(batch)\n",
        "        total_steps += 1\n",
        "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "            out = model(**batch)\n",
        "            #loss += loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
        "            loss += out.loss\n",
        "        eval_acc.update(out.logits, batch[\"labels\"])\n",
        "    # we log results at the end\n",
        "    wandb.log({\"eval/loss\": loss.item() / total_steps,\n",
        "               \"eval/accuracy\": eval_acc.compute()})\n",
        "    prompt_table(eval_dataset[:config.n_eval_samples], log=True)\n",
        "    model.train();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZXayBhip9KNi",
      "metadata": {
        "id": "ZXayBhip9KNi"
      },
      "source": [
        "모델 평가와 모델 출력을 table에 기록하는 루프를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W6Dn_4Tr9KNi",
      "metadata": {
        "id": "W6Dn_4Tr9KNi"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
        "    \"\"\"Save the model to wandb as an artifact\n",
        "    Args:\n",
        "        model (nn.Module): Model to save.\n",
        "        model_name (str): Name of the model.\n",
        "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
        "    \"\"\"\n",
        "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
        "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
        "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
        "    model.save_pretrained(file_name, safe_serialization=True)\n",
        "    # save tokenizer for easy inference\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
        "    tokenizer.save_pretrained(model_name)\n",
        "    if log:\n",
        "        at = wandb.Artifact(model_name, type=\"model\")\n",
        "        at.add_dir(file_name)\n",
        "        wandb.log_artifact(at)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NbAMUsUK9KNi",
      "metadata": {
        "id": "NbAMUsUK9KNi"
      },
      "source": [
        "## The actual Loop\n",
        "- 그래디언트 누적 및 그래디언트 스케일링\n",
        "- 샘플링 및 모델 체크포인트 저장 (이것은 매우 빠르게 훈련되므로 여러 체크포인트를 저장할 필요가 없습니다)\n",
        "- 우리는 토큰 정확도를 계산합니다, 손실보다 더 나은 지표입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TEioLioz9KNi",
      "metadata": {
        "id": "TEioLioz9KNi"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=config.project_name, # the project I am working on\n",
        "           name=config.run_name,\n",
        "           tags=[\"baseline\",\"7b\"],\n",
        "           job_type=\"train\",\n",
        "           config=config) # the Hyperparameters I want to keep track of\n",
        "\n",
        "# Training\n",
        "acc = Accuracy()\n",
        "model.train()\n",
        "train_step = 0\n",
        "for epoch in tqdm(range(config.epochs)):\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        if \"length\" in batch:\n",
        "            del batch[\"length\"]\n",
        "\n",
        "        batch = to_gpu(batch)\n",
        "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "            out = model(**batch)\n",
        "            #loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset\n",
        "            loss = out.loss / config.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "        if step%config.gradient_accumulation_steps == 0:\n",
        "            # we can log the metrics to W&B\n",
        "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
        "                       \"train/accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
        "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
        "                       \"train/global_step\": train_step})\n",
        "            optim.step()\n",
        "            scheduler.step()\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "            train_step += 1\n",
        "    validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20igM8Rn9KNi",
      "metadata": {
        "id": "20igM8Rn9KNi"
      },
      "outputs": [],
      "source": [
        "# we save the model checkpoint at the end\n",
        "#config.do_sample = True  # 샘플링을 활성화합니다.\n",
        "\n",
        "# del config.temperature  # temperature 설정을 제거합니다.\n",
        "# del config.top_p  # top_p 설정을 제거합니다.\n",
        "save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\", log=config.log_model)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6OQghgDf9KNi",
      "metadata": {
        "id": "6OQghgDf9KNi"
      },
      "source": [
        "A100에서 약 70분 정도 소요됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45KVZUwC9KNi",
      "metadata": {
        "id": "45KVZUwC9KNi"
      },
      "source": [
        "## Full Eval Dataset evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2rK1jHNy9KNj",
      "metadata": {
        "id": "2rK1jHNy9KNj"
      },
      "source": [
        "평가 데이터셋(eval_dataset)에서 모델 예측을 로그하는 테이블을 만들어 보겠습니다 (처음 250개 샘플에 대해서)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ioE1uWCh9KNj",
      "metadata": {
        "id": "ioE1uWCh9KNj"
      },
      "outputs": [],
      "source": [
        "with wandb.init(project=config.project_name, # the project I am working on\n",
        "           job_type=\"eval\",\n",
        "           config=config): # the Hyperparameters I want to keep track of\n",
        "    model.eval();\n",
        "    prompt_table(eval_dataset[:250], log=True, table_name=\"eval_predictions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zbePVRmFlP_9",
      "metadata": {
        "id": "zbePVRmFlP_9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qQ4HZlTNlQCb",
      "metadata": {
        "id": "qQ4HZlTNlQCb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j-8oIdsulQEr",
      "metadata": {
        "id": "j-8oIdsulQEr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1xUEQyJ6lQHC",
      "metadata": {
        "id": "1xUEQyJ6lQHC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DYcxzO5FlQJZ",
      "metadata": {
        "id": "DYcxzO5FlQJZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2V-eZz3ilQL4",
      "metadata": {
        "id": "2V-eZz3ilQL4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TVnmmW2clQOJ",
      "metadata": {
        "id": "TVnmmW2clQOJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5G-xfv7blQQi",
      "metadata": {
        "id": "5G-xfv7blQQi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52eH-gRllQTI",
      "metadata": {
        "id": "52eH-gRllQTI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UwBvyJsplQVj",
      "metadata": {
        "id": "UwBvyJsplQVj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jCh-S5b1dg00",
      "metadata": {
        "id": "jCh-S5b1dg00"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}