{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hukim1112/one-day-LLM-FT/blob/main/Alpaca_LLaMa_instruction_fintuning_torch_style.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3",
      "metadata": {
        "id": "3c7c21b5-4457-481f-b2cc-fb20cdcbfbe3"
      },
      "source": [
        "# From Llama to Alpaca: Finetunning and LLM with Weights & Biases\n",
        "\n",
        "ì´ Notebookì—ì„œëŠ” ì‚¬ì „ í›ˆë ¨ëœ LLama ëª¨ë¸ì„ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ë°ì´í„°ì…‹ì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •(fine-tuning)í•˜ëŠ” ë°©ë²•ì„ ë°°ìš¸ ê²ƒì…ë‹ˆë‹¤. davinci-003 (GPT-3)ìœ¼ë¡œ ìƒì„±ëœ ë°ì´í„° ëŒ€ì‹  GPT-4ë¥¼ ì‚¬ìš©í•˜ì—¬ ë”ìš± í–¥ìƒëœ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ëŠ” ì—…ë°ì´íŠ¸ëœ ë²„ì „ì˜ Alpaca ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [ê³µì‹ ì €ì¥ì†Œ í˜ì´ì§€](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM#how-good-is-the-data)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
        "\n",
        "ì´ Notebookì€ ìµœì†Œ 24GB ë©”ëª¨ë¦¬ë¥¼ ê°–ì¶˜ A100/A10 GPUê°€ í•„ìš”í•©ë‹ˆë‹¤. ë§¤ê°œë³€ìˆ˜ë¥¼ ì¡°ì •í•˜ì—¬ T4ì—ì„œ ì‹¤í–‰í•  ìˆ˜ë„ ìˆì§€ë§Œ ì‹¤í–‰ ì‹œê°„ì´ ë§¤ìš° ê¸¸ì–´ì§‘ë‹ˆë‹¤.\n",
        "\n",
        "ì´ Notebookì—ëŠ” ì—°ê´€ í”„ë¡œì íŠ¸ ë° ë³´ê³ ì„œ: [wandb](wandb.me/alpaca)ê°€ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03ef319f-bf26-4192-8951-8d536181ab67",
      "metadata": {
        "id": "03ef319f-bf26-4192-8951-8d536181ab67"
      },
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!pip install wandb\n",
        "!pip install transformers\n",
        "!pip install accelerate -U\n",
        "!pip install trl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7804f904-5746-4530-867d-c766f4501dea",
      "metadata": {
        "id": "7804f904-5746-4530-867d-c766f4501dea"
      },
      "source": [
        "## Prepare your Instruction Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da04c0a5-f481-4364-880d-10c254388987",
      "metadata": {
        "id": "da04c0a5-f481-4364-880d-10c254388987"
      },
      "source": [
        "ì•ŒíŒŒì¹´ (GPT-4 curated instructions and outputs) ë°ì´í„°ì…‹ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9",
      "metadata": {
        "id": "52ff363e-8a24-4085-9b7e-6564d106d2e9"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0665a80-6137-4a61-a3da-93bde606df04",
      "metadata": {
        "id": "b0665a80-6137-4a61-a3da-93bde606df04"
      },
      "source": [
        "ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zKGbqe450SI4",
      "metadata": {
        "id": "zKGbqe450SI4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"alpaca_gpt4_data.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6XvtiMRbF097",
      "metadata": {
        "id": "6XvtiMRbF097"
      },
      "source": [
        "ë°ì´í„°ì…‹ì˜ êµ¬ì¡°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vVWCGCLZFxsV",
      "metadata": {
        "id": "vVWCGCLZFxsV"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tkgsRp7TF5OG",
      "metadata": {
        "id": "tkgsRp7TF5OG"
      },
      "source": [
        "í•˜ë‚˜ì˜ ìƒ˜í”Œì„ í™•ì¸í•´ë´…ì‹œë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gMZA6TyhF5Wz",
      "metadata": {
        "id": "gMZA6TyhF5Wz"
      },
      "outputs": [],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zMXIcvb8HUGg",
      "metadata": {
        "id": "zMXIcvb8HUGg"
      },
      "outputs": [],
      "source": [
        "dataset['train'][9]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e596e369-56aa-4721-9271-6686eed8fb35",
      "metadata": {
        "id": "e596e369-56aa-4721-9271-6686eed8fb35"
      },
      "source": [
        "ë°ì´í„°ì…‹ì—ëŠ” ëª…ë ¹(instruction)ê³¼ ê²°ê³¼(output)ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì€ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ë˜ë¯€ë¡œ, í•œ ê°€ì§€ ë°©ë²•ì€ ë‹¨ìˆœíˆ ë‘˜ì„ ì—°ê²°(concatenate)í•˜ê³  ê·¸ ê²°ê³¼ë¥¼ í† ëŒ€ë¡œ ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ìƒì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ëŠ” ì…ë ¥ê³¼ ì¶œë ¥ ìœ„ì¹˜ë¥¼ ëª…í™•í•˜ê²Œ í‘œì‹œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d",
      "metadata": {
        "id": "ece48bbf-ddc0-4507-a733-83c5b3c1c20d"
      },
      "outputs": [],
      "source": [
        "def prompt_no_input(example):\n",
        "    return (\"Below is an instruction that describes a task. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "024aec96-40fb-4417-8e64-060a301b0f0b",
      "metadata": {
        "id": "024aec96-40fb-4417-8e64-060a301b0f0b"
      },
      "outputs": [],
      "source": [
        "row = dataset['train'][0]\n",
        "print(prompt_no_input(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037",
      "metadata": {
        "id": "8e6f6a55-fd66-4215-bee7-1a05ef91e037"
      },
      "source": [
        "ì–´ë–¤ instructionì€ input ë³€ìˆ˜ ì•ˆì— contextê°€ ë“¤ì–´ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b795343f-0356-4689-8bc6-9ac650716c8b",
      "metadata": {
        "id": "b795343f-0356-4689-8bc6-9ac650716c8b"
      },
      "outputs": [],
      "source": [
        "def prompt_input(example):\n",
        "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6",
      "metadata": {
        "id": "04d0035e-96e6-4d69-ba1f-06e0051a3db6"
      },
      "outputs": [],
      "source": [
        "row = dataset['train'][9]\n",
        "print(prompt_input(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a",
      "metadata": {
        "id": "24f74b3e-8ca2-4c57-b37f-225164c2cb5a"
      },
      "source": [
        "ì¼ë‹¨ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤. ë‚˜ì¤‘ì— ì ì ˆí•œ ì–‘ì˜ íŒ¨ë”©(padding)ê³¼ í•¨ê»˜ ê²°ê³¼ë¥¼ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f",
      "metadata": {
        "id": "cee98efa-c7ed-43a9-94bc-aad7e0da735f"
      },
      "source": [
        "inputì´ ìˆëŠ” ì¼€ì´ìŠ¤ì™€ ì—†ëŠ” ì¼€ì´ìŠ¤ë¥¼ í†µí•©í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cq3jyI8swNSO",
      "metadata": {
        "id": "Cq3jyI8swNSO"
      },
      "outputs": [],
      "source": [
        "def create_alpaca_prompt(example):\n",
        "    example['prompt'] = prompt_no_input(example) if example[\"input\"] == \"\" else prompt_input(example)\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f-H6Cd1s7E",
      "metadata": {
        "id": "70f-H6Cd1s7E"
      },
      "outputs": [],
      "source": [
        "prompt_dataset = dataset.map(create_alpaca_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DAh3RSBuIzEO",
      "metadata": {
        "id": "DAh3RSBuIzEO"
      },
      "outputs": [],
      "source": [
        "prompt_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813",
      "metadata": {
        "id": "0a4c16a0-989b-4e17-bc07-e1cc95971813"
      },
      "outputs": [],
      "source": [
        "print(prompt_dataset['train']['prompt'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc69153b-eb20-4f6d-afba-5b737d36320b",
      "metadata": {
        "id": "cc69153b-eb20-4f6d-afba-5b737d36320b"
      },
      "source": [
        "ê°€ë”ì”© ì•„ë˜ì²˜ëŸ¼ ì¢…ë£Œ í† í°(EOS)ì„ ìˆ˜ë™ìœ¼ë¡œ ì£¼ëŠ” ì˜ˆì œë“¤ì´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” Fine-tuning ì¤‘ì— ë¬¸ì œë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "351f519a-7234-4817-a6ee-4d7418f234fe",
      "metadata": {
        "id": "351f519a-7234-4817-a6ee-4d7418f234fe"
      },
      "outputs": [],
      "source": [
        "def pad_eos(example):\n",
        "    EOS_TOKEN = \"</s>\"\n",
        "    example['answer'] = f\"{example['output']}{EOS_TOKEN}\"\n",
        "    return example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aac1c00-6cc7-4b65-a972-e0ba5a2d6504",
      "metadata": {
        "id": "3aac1c00-6cc7-4b65-a972-e0ba5a2d6504"
      },
      "source": [
        "- ì˜ˆì œ í…ìŠ¤íŠ¸\n",
        "\n",
        "  \n",
        "text = \"\\<s>This is a sample sentence.\\</s>\"\n",
        "\n",
        "- í† í°í™”\n",
        "tokenized_output = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "print(tokenized_output)\n",
        "\n",
        "=> {'input_ids': tensor([[ 1, 1, 4013, 338, 263, 4559, 10541, 21106, 29879, 29958]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
        "\n",
        "- decodingì„ í•´ë³´ë©´,\n",
        "tokenizer.decode([ 1, 1, 4013, 338, 263, 4559, 10541, 21106, 29879, 29958])\n",
        "\n",
        "\"\\<s>This is a sample sentence.\\</s>\"\n",
        "\n",
        "ì˜ˆì œë¥¼ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯ì´\n",
        "í•˜ì§€ë§Œ \\</s>ì´ 29879, 29958ë¡œ ì¸ì½”ë”© ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì¦‰ ì˜ˆìƒì¹˜ ëª»í•œ id ê°’ìœ¼ë¡œ ë³€ê²½ë˜ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "ìš°ë¦¬ê°€ ê¸°ëŒ€í•œ EOS token idëŠ” 2ë²ˆì…ë‹ˆë‹¤. ì‹¤ì œë¡œ token id 2ì˜ decoding ê²°ê³¼ëŠ” \\</s>ê°€ ë§ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì •ë¦¬í•˜ìë©´ ë¬¸ìì—´ì€ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ tokenization ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— special tokenì˜ ê²½ìš° ì •í•´ì§„ token idë¡œ ë³€ê²½ë˜ëŠ” ê²Œ ì¤‘ìš”í•˜ë¯€ë¡œ ìœ ì˜í•´ì•¼ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4",
      "metadata": {
        "id": "06177a5c-84ff-4be3-8d2b-6a483c8ae5e4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qul4fye5wNT-",
      "metadata": {
        "id": "Qul4fye5wNT-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "063Ax0luKJgy",
      "metadata": {
        "id": "063Ax0luKJgy"
      },
      "source": [
        "ìµœì¢…ì ìœ¼ë¡œ ìœ ì € promptì™€ ëª¨ë¸ answerë¥¼ í•©ì¹©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802",
      "metadata": {
        "id": "e923b9bb-ced2-44f9-88f3-1c7690dde802"
      },
      "outputs": [],
      "source": [
        "def get_example(example):\n",
        "    example['example'] = example['prompt'] + example['output']\n",
        "    return example\n",
        "\n",
        "\n",
        "final_dataset = prompt_dataset.map(get_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bad92d95-23d5-474c-9271-59948d5dcbb0",
      "metadata": {
        "id": "bad92d95-23d5-474c-9271-59948d5dcbb0"
      },
      "source": [
        "ì´ê²ƒì´ ëª¨ë¸ì´ ë³´ê³  ë°°ìš¸ í•„ìš”ê°€ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R16Ge7dqJ3oz",
      "metadata": {
        "id": "R16Ge7dqJ3oz"
      },
      "outputs": [],
      "source": [
        "print(final_dataset['train']['example'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd",
      "metadata": {
        "id": "5270873e-53d4-492b-bdce-b4d8ed8084bd"
      },
      "source": [
        "## Converting text to numbers: Tokenizer\n",
        "\n",
        "ìš°ë¦¬ëŠ” ë°ì´í„°ì…‹ì„ í† í°ë“¤ë¡œ ë³€í™˜í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì€ transformersì˜ tokenizerë¡œ ì‰½ê²Œ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "720c707b-3bce-4164-b8c1-3c3122200c39",
      "metadata": {
        "id": "720c707b-3bce-4164-b8c1-3c3122200c39"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Gd05dHqPL2Ie",
      "metadata": {
        "id": "Gd05dHqPL2Ie"
      },
      "outputs": [],
      "source": [
        "model_id = 'NousResearch/Llama-2-7b-chat-hf'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ZKPEegTL0p1",
      "metadata": {
        "id": "5ZKPEegTL0p1"
      },
      "outputs": [],
      "source": [
        "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
        "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wInLJDPBL718",
      "metadata": {
        "id": "wInLJDPBL718"
      },
      "source": [
        "ë§ì€ íŠœí† ë¦¬ì–¼ì´ ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ì„ ì¶”ì²œí•˜ì§€ë§Œ,\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "ì´ ê²½ìš° í•™ìŠµ ì‹œ pad tokenì´ ë¬´ì‹œë˜ë©´ì„œ eos token ë¬´ì‹œë˜ë©´ì„œ ëª¨ë¸ì´ ë¬¸ì¥ì˜ ëì„ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›Œì§€ëŠ” ë¬¸ì œê°€ ìˆìŒ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DREOc9z0MKfw",
      "metadata": {
        "id": "DREOc9z0MKfw"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token_id = 0 #ì¶”ë¡  ì‹œì—ëŠ” tokenizer.eos_token_idë¡œ ì§€ì •í•´ë„ ìƒê´€ ì—†ìŒ.\n",
        "\n",
        "print(f\"Pad Token id: {tokenizer.pad_token_id} and Pad Token: {tokenizer.pad_token}\")\n",
        "print(f\"EOS Token id: {tokenizer.eos_token_id} and EOS Token: {tokenizer.eos_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "337fedfd-e238-4e86-a96b-24dfeed11f8a",
      "metadata": {
        "id": "337fedfd-e238-4e86-a96b-24dfeed11f8a"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a99cdc97-0cb6-4e5e-ac9e-27c7cf22d46d",
      "metadata": {
        "id": "a99cdc97-0cb6-4e5e-ac9e-27c7cf22d46d"
      },
      "outputs": [],
      "source": [
        "tokenizer.add_eos_token = True # ì´ì œ tokenizerëŠ” eos tokenì„ ìë™ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "461efa85-67f3-4c12-a7d4-d4e6deb020b2",
      "metadata": {
        "id": "461efa85-67f3-4c12-a7d4-d4e6deb020b2"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6dd79e4-b4da-4d5f-9c41-404bea2f9160",
      "metadata": {
        "id": "c6dd79e4-b4da-4d5f-9c41-404bea2f9160"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode([1, 1619, 15729, 526, 2675, 4549, 29991, 2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c69466-f7e6-45b8-a167-718c80cedc0f",
      "metadata": {
        "id": "20c69466-f7e6-45b8-a167-718c80cedc0f"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\", padding='max_length', max_length=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb1e5e29-254a-4dbf-ac02-ea6bb2a16e77",
      "metadata": {
        "id": "cb1e5e29-254a-4dbf-ac02-ea6bb2a16e77"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(\"My experiments are going strong!\",\n",
        "                 padding='max_length',\n",
        "                 max_length=10,\n",
        "                 return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a89a61d-34b7-4a56-b1d8-98ebcf3384d7",
      "metadata": {
        "id": "0a89a61d-34b7-4a56-b1d8-98ebcf3384d7"
      },
      "outputs": [],
      "source": [
        "tokenizer([\"My experiments are going strong!\",\n",
        "           \"I love Llamas\"],\n",
        "          padding='max_length',\n",
        "          # padding='longest',\n",
        "          max_length=10,\n",
        "          return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qJ__Hh4Ofx4O",
      "metadata": {
        "id": "qJ__Hh4Ofx4O"
      },
      "outputs": [],
      "source": [
        "x = tokenizer([\"My experiments are going strong!\",\n",
        "           \"I love Llamas\"],\n",
        "          padding='max_length',\n",
        "          # padding='longest',\n",
        "          max_length=10,\n",
        "          return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lt4qnlmMf0KO",
      "metadata": {
        "id": "lt4qnlmMf0KO"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(x['input_ids'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BAjWPwxB4Csz",
      "metadata": {
        "id": "BAjWPwxB4Csz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IK8RnHwg4CvV",
      "metadata": {
        "id": "IK8RnHwg4CvV"
      },
      "outputs": [],
      "source": [
        "for i, example in enumerate(final_dataset['train']['example'][0:3]):\n",
        "    print(f\"---------{i+1}ë²ˆì§¸ ë°ì´í„° ìƒ˜í”Œ--------------\")\n",
        "    print(example)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4RT8bdJO3Nd2",
      "metadata": {
        "id": "4RT8bdJO3Nd2"
      },
      "source": [
        "## Data collator\n",
        "\n",
        "Causal language modelingì„ ìœ„í•´ ìš°ë¦¬ëŠ” ë™ì  ë§ˆìŠ¤í‚¹ ëª¨ë“œë¥¼ offí•œ DataCollatorForLanguageModeling(tokenizer, mlm=False)ë¥¼ ì‚¬ìš©í•˜ì—¬ GPT ê³„ì—´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZTODLkVQ4SIM",
      "metadata": {
        "id": "ZTODLkVQ4SIM"
      },
      "source": [
        "### DataCollatorForLanguageModeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Pt_dQmaIl5t",
      "metadata": {
        "id": "9Pt_dQmaIl5t"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "causal_model_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KIxpzTMoIl8S",
      "metadata": {
        "id": "KIxpzTMoIl8S"
      },
      "outputs": [],
      "source": [
        "out = causal_model_collator([tokenizer(example) for example in final_dataset['train']['example'][:1]])\n",
        "for key in out:\n",
        "    print(f\"{key} : {out[key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tQA-NjC6Il-d",
      "metadata": {
        "id": "tQA-NjC6Il-d"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(out['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QI5hBhiO4amK",
      "metadata": {
        "id": "QI5hBhiO4amK"
      },
      "source": [
        "### ë¯¸ì„¸ì¡°ì •ì„ ìœ„í•œ DataCollatorForCompletionOnlyLM ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "m_dp8U173ren",
      "metadata": {
        "id": "m_dp8U173ren"
      },
      "source": [
        "DataCollatorForLanguageModelingë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì€ ì‚¬ìš©ìì˜ ì…ë ¥ ì²« ë²ˆì§¸ í† í°ë¶€í„° ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹¤ì œë¡œ ì›í•˜ëŠ” ê²ƒì€ ëª¨ë¸ì´ ëª…ë ¹ì–´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ë²•ì„ ë°°ìš°ë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë§ˆìŠ¤í‚¹í•˜ì—¬, ëª¨ë¸ í•™ìŠµ ì¤‘ì— ì´ ì…ë ¥ë“¤ì´ ì†ì‹¤(loss)ì— ê¸°ì—¬í•˜ì§€ ì•Šë„ë¡ í•´ì•¼ í•©ë‹ˆë‹¤. ëª…ë ¹ì–´ LLM ë¯¸ì„¸ ì¡°ì •ì—ì„œë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ëª…ë ¹ì–´ì˜ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•˜ë ¤ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ëª…ë ¹ì–´ë¥¼ ë§ˆìŠ¤í‚¹í•˜ê³  ëª¨ë¸ì´ ì‘ë‹µì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bKd3LT1mNX47",
      "metadata": {
        "id": "bKd3LT1mNX47"
      },
      "outputs": [],
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "\n",
        "response_template = \"Response:\"\n",
        "completion_only_collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hFM7gc_-NX7q",
      "metadata": {
        "id": "hFM7gc_-NX7q"
      },
      "outputs": [],
      "source": [
        "out = completion_only_collator([tokenizer(example) for example in final_dataset['train']['example'][:1]])\n",
        "for key in out:\n",
        "    print(f\"{key} : {out[key]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RggmKG-kNX-Z",
      "metadata": {
        "id": "RggmKG-kNX-Z"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(out['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iSnO_gW8PL26",
      "metadata": {
        "id": "iSnO_gW8PL26"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d8c8e9-73b2-4c28-846d-3ae850134a18",
      "metadata": {
        "id": "c8d8c8e9-73b2-4c28-846d-3ae850134a18"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "CZm7t1RL4L80",
      "metadata": {
        "id": "CZm7t1RL4L80"
      },
      "source": [
        "## ë°ì´í„° ìŠ¤í”Œë¦¿"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3mAGmEVM0SLD",
      "metadata": {
        "id": "3mAGmEVM0SLD"
      },
      "outputs": [],
      "source": [
        "train_test_split = final_dataset['train'].train_test_split(test_size=1000)\n",
        "# x = final_dataset['train'].train_test_split(test_size=100)\n",
        "# train_test_split = x['test'].train_test_split(test_size=20)\n",
        "# train_test_split\n",
        "# batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MUz4MPz_0SNR",
      "metadata": {
        "id": "MUz4MPz_0SNR"
      },
      "outputs": [],
      "source": [
        "# ë¶„í• ëœ ë°ì´í„°ì…‹ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "print(f\"í›ˆë ¨ ë°ì´í„° ê°œìˆ˜: {len(train_dataset)}\")\n",
        "print(f\"í‰ê°€ ë°ì´í„° ê°œìˆ˜: {len(eval_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I3k3xqsZ4kna",
      "metadata": {
        "id": "I3k3xqsZ4kna"
      },
      "source": [
        "## Tokenization ì „ì²˜ë¦¬ì™€ Dataloader ì—°ê²°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tSWw-mqDg47C",
      "metadata": {
        "id": "tSWw-mqDg47C"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = train_dataset.map(lambda x : tokenizer(x['example']))\n",
        "tokenized_eval_dataset = eval_dataset.map(lambda x : tokenizer(x['example']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jG4Zs-rEh--k",
      "metadata": {
        "id": "jG4Zs-rEh--k"
      },
      "outputs": [],
      "source": [
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['instruction', 'input', 'output', 'prompt', 'example'])\n",
        "tokenized_eval_dataset = tokenized_eval_dataset.remove_columns(['instruction', 'input', 'output', 'prompt', 'example'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aqezXdMkgfsK",
      "metadata": {
        "id": "aqezXdMkgfsK"
      },
      "source": [
        "í† í°í™”ëœ ë°ì´í„°ì…‹ì„ dataloaderì™€ ì—°ê²°í•´ì¤ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vIxMYiynyqhE",
      "metadata": {
        "id": "vIxMYiynyqhE"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from transformers import default_data_collator\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "batch_size = 16  # I have an A100 GPU with 40GB of RAM ğŸ˜\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=causal_model_collator,\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_eval_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=causal_model_collator,\n",
        "    shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BG6XMsRcyqjT",
      "metadata": {
        "id": "BG6XMsRcyqjT"
      },
      "outputs": [],
      "source": [
        "b = next(iter(train_dataloader))\n",
        "b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eTep0z2lyqlh",
      "metadata": {
        "id": "eTep0z2lyqlh"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(b[\"input_ids\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XVlqlKTlyqoE",
      "metadata": {
        "id": "XVlqlKTlyqoE"
      },
      "outputs": [],
      "source": [
        "# -100ì„ ì œì™¸í•œ í† í°ë§Œ í•„í„°ë§\n",
        "valid_labels = [token_id for token_id in b[\"labels\"][0] if token_id != -100]\n",
        "\n",
        "# ìœ íš¨í•œ í† í°ì„ decode\n",
        "decoded_text = tokenizer.decode(valid_labels)\n",
        "\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sNqlCQ3UlP7V",
      "metadata": {
        "id": "sNqlCQ3UlP7V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0zJKkvf7lP9w",
      "metadata": {
        "id": "0zJKkvf7lP9w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "RDCzzQ-29KNh",
      "metadata": {
        "id": "RDCzzQ-29KNh"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hxeYSaMC9KNh",
      "metadata": {
        "id": "hxeYSaMC9KNh"
      },
      "source": [
        "ë‹¤ìŒê³¼ ê°™ì´ ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ê´€ë¦¬í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V__oSZ799KNh",
      "metadata": {
        "id": "V__oSZ799KNh"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "\n",
        "gradient_accumulation_steps = 2\n",
        "\n",
        "config = SimpleNamespace(\n",
        "    project_name='meta-llama-ft-alpaca',\n",
        "    run_name='Causal-language-modeling',\n",
        "    model_id=model_id,\n",
        "    dataset_name=\"alpaca-gpt4\",\n",
        "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
        "    n_freeze=24,  # How many layers we don't train, LLama 7B has 32.\n",
        "    lr=2e-4,\n",
        "    n_eval_samples=10, # How many samples to generate on validation\n",
        "    epochs=3,  # we do 3 pasess over the dataset.\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
        "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training\n",
        "    log_model=False,  # upload the model to W&B?\n",
        "    gradient_checkpointing = True,  # saves even more memory\n",
        "    freeze_embed = True,  # why train this? let's keep them frozen â„ï¸\n",
        "    seed=seed,\n",
        ")\n",
        "\n",
        "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JOW-3sjl9KNh",
      "metadata": {
        "id": "JOW-3sjl9KNh"
      },
      "outputs": [],
      "source": [
        "print(f\"We will train for {config.total_train_steps} steps and evaluate every epoch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LKj0gF-t9KNh",
      "metadata": {
        "id": "LKj0gF-t9KNh"
      },
      "source": [
        "pretrained modelì„ ê°€ì ¸ì˜µë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KsKyp3Ao9KNh",
      "metadata": {
        "id": "KsKyp3Ao9KNh"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    config.model_id,\n",
        "    device_map=0,\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    use_cache=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IutRmWXX9KNh",
      "metadata": {
        "id": "IutRmWXX9KNh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def param_count(m):\n",
        "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
        "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
        "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
        "    return params, trainable_params\n",
        "\n",
        "params, trainable_params = param_count(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C1DNSEko9KNh",
      "metadata": {
        "id": "C1DNSEko9KNh"
      },
      "source": [
        "ì „ì²´ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²ƒì€ ê°•ë ¥í•œ ì—°ì‚°ë ¥ê³¼ ë©”ëª¨ë¦¬ë¥¼ í•„ìš”ë¡œí•˜ê¸° ë•Œë¬¸ì— ìš°ë¦¬ëŠ” 8ê°œì˜ layerë¥¼ íŠœë‹í•  ê²ƒ ì…ë‹ˆë‹¤. LLamaëŠ” ì´ 32ê°œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vr39Gdt19KNi",
      "metadata": {
        "id": "Vr39Gdt19KNi"
      },
      "outputs": [],
      "source": [
        "# freeze layers (disable gradients)\n",
        "for param in model.parameters(): param.requires_grad = False\n",
        "for param in model.lm_head.parameters(): param.requires_grad = True\n",
        "for param in model.model.layers[config.n_freeze:].parameters(): param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JmHxbYJ29KNi",
      "metadata": {
        "id": "JmHxbYJ29KNi"
      },
      "outputs": [],
      "source": [
        "# Just freeze embeddings for small memory decrease\n",
        "if config.freeze_embed:\n",
        "    model.model.embed_tokens.weight.requires_grad_(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ReVioN69KNi",
      "metadata": {
        "id": "6ReVioN69KNi"
      },
      "source": [
        "ë˜í•œ ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…ì„ ì‚¬ìš©í•˜ì—¬ ë” ë§ì´ ì €ì¥í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤(ì´ê²ƒì€ í›ˆë ¨ì„ ëŠë¦¬ê²Œ ë§Œë“¤ì§€ë§Œ, ì–¼ë§ˆë‚˜ ëŠë ¤ì§ˆì§€ëŠ” ì—¬ëŸ¬ë¶„ì˜ íŠ¹ì • ì„¤ì •ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤). ëŒ€ìš©ëŸ‰ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë§ì¶”ëŠ” ë°©ë²•ì— ëŒ€í•´ í—ˆê¹…í˜ì´ìŠ¤ ì›¹ì‚¬ì´íŠ¸ì— [ì¢‹ì€ ì•„í‹°í´](https://huggingface.co/docs/transformers/v4.18.0/en/performance)ì´ ìˆìœ¼ë‹ˆ í™•ì¸í•´ ë³´ì‹œê¸¸ ê¶Œì¥í•©ë‹ˆë‹¤!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vpmm8SI_9KNi",
      "metadata": {
        "id": "vpmm8SI_9KNi"
      },
      "outputs": [],
      "source": [
        "# save more memory\n",
        "if config.gradient_checkpointing:\n",
        "    model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25QIm0yQ9KNi",
      "metadata": {
        "id": "25QIm0yQ9KNi"
      },
      "outputs": [],
      "source": [
        "params, trainable_params = param_count(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ol4k95ZN9KNi",
      "metadata": {
        "id": "Ol4k95ZN9KNi"
      },
      "source": [
        "### Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YRcKwYyI9KNi",
      "metadata": {
        "id": "YRcKwYyI9KNi"
      },
      "outputs": [],
      "source": [
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9,0.99), eps=1e-5)\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optim,\n",
        "    num_training_steps=config.total_train_steps,\n",
        "    num_warmup_steps=config.total_train_steps // 10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xAiICm3J9KNi",
      "metadata": {
        "id": "xAiICm3J9KNi"
      },
      "source": [
        "## Testing during training\n",
        "\n",
        "ê±°ì˜ ë‹¤ ì™”ìŠµë‹ˆë‹¤, ì´ì œ ëª¨ë¸ì—ì„œ ìƒ˜í”Œë§í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ê°€ë” ëª¨ë¸ì´ ì¶œë ¥í•˜ëŠ” ê²ƒì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•´ ë´…ì‹œë‹¤! ê°„ë‹¨í•˜ê²Œ ëª¨ë¸.generate ë©”ì†Œë“œë¥¼ ê°ì‹¸ ë³´ê² ìŠµë‹ˆë‹¤. GenerationConfigì—ì„œ ê¸°ë³¸ ìƒ˜í”Œë§ ë§¤ê°œë³€ìˆ˜ë¥¼ ê°€ì ¸ì™€ í•´ë‹¹ ëª¨ë¸ IDë¥¼ ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qMQZ9Gg_9KNi",
      "metadata": {
        "id": "qMQZ9Gg_9KNi"
      },
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
        "test_config = SimpleNamespace(\n",
        "    max_new_tokens=256,\n",
        "    gen_config=gen_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hse5ft8Y9KNi",
      "metadata": {
        "id": "Hse5ft8Y9KNi"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, max_new_tokens=test_config.max_new_tokens, gen_config=gen_config):\n",
        "    tokenizer.add_eos_token = False\n",
        "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
        "    with torch.inference_mode():\n",
        "        output = model.generate(tokenized_prompt,\n",
        "                            max_new_tokens=max_new_tokens,\n",
        "                            generation_config=gen_config)\n",
        "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44aNuoyS9KNi",
      "metadata": {
        "id": "44aNuoyS9KNi"
      },
      "source": [
        "LoL ğŸ¤·"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6PyMJtNA9KNi",
      "metadata": {
        "id": "6PyMJtNA9KNi"
      },
      "outputs": [],
      "source": [
        "prompt = eval_dataset[14][\"prompt\"]\n",
        "print(prompt + generate(prompt, 128))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5123cec1-63f1-4b47-b773-021842b2dd57",
      "metadata": {
        "id": "5123cec1-63f1-4b47-b773-021842b2dd57"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "oiL1PIcT9KNi",
      "metadata": {
        "id": "oiL1PIcT9KNi"
      },
      "source": [
        "ìš°ë¦¬ëŠ” ê·¸ ê²°ê³¼ë¥¼ n ë‹¨ê³„ë§ˆë‹¤ í”„ë¡œì íŠ¸ì— í…Œì´ë¸”ë¡œ ê¸°ë¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SxZQ-jAh9KNi",
      "metadata": {
        "id": "SxZQ-jAh9KNi"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def prompt_table(examples, log=False, table_name=\"predictions\"):\n",
        "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"output\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
        "\n",
        "    for prompt, gpt4_output in tqdm(zip(examples['prompt'], examples['output']), leave=False):\n",
        "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
        "        table.add_data(prompt, out, prompt+out, gpt4_output, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
        "    if log:\n",
        "        wandb.log({table_name:table})\n",
        "    return table\n",
        "\n",
        "def to_gpu(tensor_dict):\n",
        "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
        "\n",
        "class Accuracy:\n",
        "    \"A simple Accuracy function compatible with HF models\"\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "        self.tp = 0.\n",
        "    def update(self, logits, labels):\n",
        "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
        "        tp = (logits == labels).sum()\n",
        "        self.count += len(logits)\n",
        "        self.tp += tp\n",
        "        return tp / len(logits)\n",
        "    def compute(self):\n",
        "        return self.tp / self.count"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vvUS5CJV9KNi",
      "metadata": {
        "id": "vvUS5CJV9KNi"
      },
      "source": [
        "ì›í•˜ì‹ ë‹¤ë©´ ê²€ì¦ì„ ë¹ ë¥´ê²Œ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œ í…Œì´ë¸”ì„ ìƒì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tj57fWkm9KNi",
      "metadata": {
        "id": "Tj57fWkm9KNi"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate():\n",
        "    model.eval();\n",
        "    eval_acc = Accuracy()\n",
        "    loss, total_steps = 0., 0\n",
        "    for step, batch in enumerate(pbar:=tqdm(eval_dataloader, leave=False)):\n",
        "        if \"length\" in batch:\n",
        "            del batch[\"length\"]\n",
        "        pbar.set_description(f\"doing validation\")\n",
        "        batch = to_gpu(batch)\n",
        "        total_steps += 1\n",
        "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "            out = model(**batch)\n",
        "            #loss += loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
        "            loss += out.loss\n",
        "        eval_acc.update(out.logits, batch[\"labels\"])\n",
        "    # we log results at the end\n",
        "    wandb.log({\"eval/loss\": loss.item() / total_steps,\n",
        "               \"eval/accuracy\": eval_acc.compute()})\n",
        "    prompt_table(eval_dataset[:config.n_eval_samples], log=True)\n",
        "    model.train();"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZXayBhip9KNi",
      "metadata": {
        "id": "ZXayBhip9KNi"
      },
      "source": [
        "ëª¨ë¸ í‰ê°€ì™€ ëª¨ë¸ ì¶œë ¥ì„ tableì— ê¸°ë¡í•˜ëŠ” ë£¨í”„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W6Dn_4Tr9KNi",
      "metadata": {
        "id": "W6Dn_4Tr9KNi"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
        "    \"\"\"Save the model to wandb as an artifact\n",
        "    Args:\n",
        "        model (nn.Module): Model to save.\n",
        "        model_name (str): Name of the model.\n",
        "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
        "    \"\"\"\n",
        "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
        "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
        "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
        "    model.save_pretrained(file_name, safe_serialization=True)\n",
        "    # save tokenizer for easy inference\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
        "    tokenizer.save_pretrained(model_name)\n",
        "    if log:\n",
        "        at = wandb.Artifact(model_name, type=\"model\")\n",
        "        at.add_dir(file_name)\n",
        "        wandb.log_artifact(at)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NbAMUsUK9KNi",
      "metadata": {
        "id": "NbAMUsUK9KNi"
      },
      "source": [
        "## The actual Loop\n",
        "- ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë° ê·¸ë˜ë””ì–¸íŠ¸ ìŠ¤ì¼€ì¼ë§\n",
        "- ìƒ˜í”Œë§ ë° ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì´ê²ƒì€ ë§¤ìš° ë¹ ë¥´ê²Œ í›ˆë ¨ë˜ë¯€ë¡œ ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤)\n",
        "- ìš°ë¦¬ëŠ” í† í° ì •í™•ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤, ì†ì‹¤ë³´ë‹¤ ë” ë‚˜ì€ ì§€í‘œì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TEioLioz9KNi",
      "metadata": {
        "id": "TEioLioz9KNi"
      },
      "outputs": [],
      "source": [
        "wandb.init(project=config.project_name, # the project I am working on\n",
        "           name=config.run_name,\n",
        "           tags=[\"baseline\",\"7b\"],\n",
        "           job_type=\"train\",\n",
        "           config=config) # the Hyperparameters I want to keep track of\n",
        "\n",
        "# Training\n",
        "acc = Accuracy()\n",
        "model.train()\n",
        "train_step = 0\n",
        "for epoch in tqdm(range(config.epochs)):\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "        if \"length\" in batch:\n",
        "            del batch[\"length\"]\n",
        "\n",
        "        batch = to_gpu(batch)\n",
        "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
        "            out = model(**batch)\n",
        "            #loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset\n",
        "            loss = out.loss / config.gradient_accumulation_steps\n",
        "            loss.backward()\n",
        "        if step%config.gradient_accumulation_steps == 0:\n",
        "            # we can log the metrics to W&B\n",
        "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
        "                       \"train/accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
        "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
        "                       \"train/global_step\": train_step})\n",
        "            optim.step()\n",
        "            scheduler.step()\n",
        "            optim.zero_grad(set_to_none=True)\n",
        "            train_step += 1\n",
        "    validate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20igM8Rn9KNi",
      "metadata": {
        "id": "20igM8Rn9KNi"
      },
      "outputs": [],
      "source": [
        "# we save the model checkpoint at the end\n",
        "#config.do_sample = True  # ìƒ˜í”Œë§ì„ í™œì„±í™”í•©ë‹ˆë‹¤.\n",
        "\n",
        "# del config.temperature  # temperature ì„¤ì •ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
        "# del config.top_p  # top_p ì„¤ì •ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
        "save_model(model, model_name=config.model_id.replace(\"/\", \"_\"), models_folder=\"models/\", log=config.log_model)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6OQghgDf9KNi",
      "metadata": {
        "id": "6OQghgDf9KNi"
      },
      "source": [
        "A100ì—ì„œ ì•½ 70ë¶„ ì •ë„ ì†Œìš”ë©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45KVZUwC9KNi",
      "metadata": {
        "id": "45KVZUwC9KNi"
      },
      "source": [
        "## Full Eval Dataset evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2rK1jHNy9KNj",
      "metadata": {
        "id": "2rK1jHNy9KNj"
      },
      "source": [
        "í‰ê°€ ë°ì´í„°ì…‹(eval_dataset)ì—ì„œ ëª¨ë¸ ì˜ˆì¸¡ì„ ë¡œê·¸í•˜ëŠ” í…Œì´ë¸”ì„ ë§Œë“¤ì–´ ë³´ê² ìŠµë‹ˆë‹¤ (ì²˜ìŒ 250ê°œ ìƒ˜í”Œì— ëŒ€í•´ì„œ)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ioE1uWCh9KNj",
      "metadata": {
        "id": "ioE1uWCh9KNj"
      },
      "outputs": [],
      "source": [
        "with wandb.init(project=config.project_name, # the project I am working on\n",
        "           job_type=\"eval\",\n",
        "           config=config): # the Hyperparameters I want to keep track of\n",
        "    model.eval();\n",
        "    prompt_table(eval_dataset[:250], log=True, table_name=\"eval_predictions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zbePVRmFlP_9",
      "metadata": {
        "id": "zbePVRmFlP_9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qQ4HZlTNlQCb",
      "metadata": {
        "id": "qQ4HZlTNlQCb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j-8oIdsulQEr",
      "metadata": {
        "id": "j-8oIdsulQEr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1xUEQyJ6lQHC",
      "metadata": {
        "id": "1xUEQyJ6lQHC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DYcxzO5FlQJZ",
      "metadata": {
        "id": "DYcxzO5FlQJZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2V-eZz3ilQL4",
      "metadata": {
        "id": "2V-eZz3ilQL4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TVnmmW2clQOJ",
      "metadata": {
        "id": "TVnmmW2clQOJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5G-xfv7blQQi",
      "metadata": {
        "id": "5G-xfv7blQQi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52eH-gRllQTI",
      "metadata": {
        "id": "52eH-gRllQTI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UwBvyJsplQVj",
      "metadata": {
        "id": "UwBvyJsplQVj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jCh-S5b1dg00",
      "metadata": {
        "id": "jCh-S5b1dg00"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "PyTorch 2.2 (NGC 23.11/Python 3.10) on Backend.AI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}